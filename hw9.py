# -*- coding: utf-8 -*-
"""hw9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tWr_0sUh6h_Zve4_lEc8cfNaHscYtNaB
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!wget -q https://downloads.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz 

!tar -xvf spark-3.2.1-bin-hadoop3.2.tgz

!pip install pyspark
!pip install -q findspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import col  
from pyspark.sql.functions import lit 
from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml import Pipeline
import pandas as pd 
import numpy as np 
import time 
import matplotlib.pyplot as plt

truth_cols = ['userID','trackID','ground_truth']
truth_df = pd.read_csv('3_new_data_for_HW9.txt',sep='|',names=truth_cols)
truth_df

cols_of_scores = ['userID','trackID',"album_score","artist_score"]
scores_df = pd.read_csv('4_output1.txt',sep='|',names=cols_of_scores)
scores_df

rating_df = truth_df.merge(scores_df, on=["userID","trackID"]).fillna(0)
rating_df

rating_df.to_csv("5_scores.csv",index=None)

spark = SparkSession.builder.appName("hw9").getOrCreate()

spark

ratings_df = spark.read.csv("5_scores.csv", header=True, inferSchema=True)
ratings_df

ratings_df.count()

col_ratings = ratings_df.columns
col_ratings

pd.DataFrame(ratings_df.take(6000),columns=col_ratings).groupby('ground_truth').count()

ratings_df.printSchema()

ratings_df

ratings_df = ratings_df.withColumn('ground_truth',ratings_df['ground_truth'].cast('string'))

ratings_df

features=['album_score','artist_score']
stages=[]
inputs = features 
#Use vector assembler 
Vassembler = VectorAssembler(inputCols=inputs,outputCol='features')
stages+=[Vassembler]

stages

#use StringIndexer to encode string columns of labels 
label_column = 'ground_truth'
label_string = StringIndexer(inputCol=label_column,outputCol="label")
stages+=[label_string]

pipeline = Pipeline(stages=stages)
pmodel = pipeline.fit(ratings_df)
train_df = pmodel.transform(ratings_df)

sel_col = ['label','features'] + col_ratings 
train_df = train_df.select(sel_col)
train_df.printSchema()

pd.DataFrame(train_df.take(5),columns=train_df.columns).transpose()

train_df, test_df = train_df.randomSplit([0.7,0.3],seed=2022)

print("training dataset count: " + str(train_df.count()))
print("test dataset count: " + str(test_df.count()))

prediction_df = spark.read.csv('4_output1.txt', sep='|', inferSchema=True)

prediction_df.count()

prediction_df = prediction_df.withColumnRenamed("_c0", "userID").withColumnRenamed("_c1", "trackID").withColumnRenamed("_c2", "albumScore").withColumnRenamed("_c3", "artistScore")

prediction_columns = prediction_df.columns
prediction_columns

prediction_df = prediction_df.withColumn('prediction', lit('0'))

pd.DataFrame(prediction_df.take(5), columns=prediction_df.columns).transpose()

prediction_df.printSchema()

feature_col = ['albumScore', 'artistScore']
stages = []
assembler_inputs = feature_col
assem = VectorAssembler(inputCols=assembler_inputs, outputCol='features')    # merges multiple columns into a vector column
stages += [assem]

label_col = 'prediction'
label_string_idx = StringIndexer(inputCol=label_col, outputCol='label')
stages += [label_string_idx]

pred_pipeline = Pipeline(stages=stages)                       
pred_pipeline_model = pred_pipeline.fit(prediction_df)  
prediction_df = pred_pipeline_model.transform(prediction_df)

selected_col = ['label', 'features'] + prediction_columns
prediction_df = prediction_df.select(selected_col)
prediction_df.printSchema()

pd.DataFrame(prediction_df.take(5), columns=prediction_df.columns).transpose()

#Model 1: Logistic Regression
start = time.time()
lr = LogisticRegression(featuresCol='features',labelCol='label',maxIter=100)
lr_model = lr.fit(train_df) 
end = time.time()
elapsed = end-start
print("Done Time taken: " + str(elapsed) + " secs")

lr_model.coefficients

beta = np.sort(lr_model.coefficients)
beta

plt.plot(beta)
plt.ylabel('Beta Coefficients')
plt.show()

training_sum = lr_model.summary

roc = training_sum.roc.toPandas()

roc

plt.plot(roc.FPR,roc.TPR)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title('Logistic Regression')
plt.show()
print("Training Set AUC = " + str(training_sum.areaUnderROC))

predictions = lr_model.transform(test_df)    # transform test_df with logistic regression model
predictions.select('userID', 'trackID', 'label', 'probability', 'rawPrediction', 'prediction').show(12)

sort_predictions = predictions.select('userID', 'trackID', 'label', 'probability', 'rawPrediction', 'prediction').sort(col('userID').asc(), col('probability').desc())
sort_predictions.show(6)

logistic_predictions = lr_model.transform(prediction_df)    # transform prediction_df with logistic regression model
logistic_predictions.select('userID', 'trackID', 'probability', 'rawPrediction', 'prediction').show(12)

sort_logistic_predictions = logistic_predictions.select('userID', 'trackID', 'probability', 'rawPrediction', 'prediction').sort(col('userID').asc(), col('probability').desc())
sort_logistic_predictions.show(6)

pd_sort_logistic_predictions = sort_logistic_predictions.toPandas().fillna(0.0)    # create a pandas df

pd_sort_logistic_predictions

columns_to_write = ['userID', 'trackID']
pd_sort_logistic_predictions.to_csv('logisticregression.csv', index=False, header=None, columns=columns_to_write)    # write to csv (without headers)

f_lr_predictions = open('logisticregression.csv')                  
f_lr_final_predictions = open('lr_final_predictions.csv', 'w')

# Initialize some values
last_user_id = -1
track_id_out_vec = [0] * 6

start_time = time.time()

# Go through each line of the predictions file
for line in f_lr_predictions:
    arr_out = line.strip().split(',')    # remove any spaces/new lines and create list 
    user_id_out = arr_out[0]             # set user
    track_id_out = arr_out[1]            # set track
    
    if user_id_out != last_user_id:             # if new user reached
        i = 0                                   # reset i
        
    track_id_out_vec[i] = track_id_out          # add trackID to trackID array
        
    i = i + 1                    # increment i
    last_user_id = user_id_out   # set last_user_id as current userID
    
    if i == 6:                               # if last entry for current user reached
        # Here we set the predictions 
        predictions = np.ones(shape=(6)) # initialize numpy array for predictions
        for index in range(0, 3):            
            predictions[index] = 0           # set first 3 values in array to 0 (other 3 are 1)
        
        # Here we write to the final predictions file for the 6 track predictions for the current user
        for ii in range(0, 6):         
            out_str = str(user_id_out) + '_' + str(track_id_out_vec[ii]) + ',' + str(int(predictions[ii]))
            f_lr_final_predictions.write(out_str + '\n')

        
end_time = time.time()
elapsed_time = end_time - start_time
print(f'Done! Time elapsed - {elapsed_time:.2f} seconds.')

f_lr_predictions.close()
f_lr_final_predictions.close()

# Part 3: Decision Tree Classifier 

decision_tree = DecisionTreeClassifier(featuresCol='features', labelCol='label', maxDepth=3)
dt_model = decision_tree.fit(train_df)
predictions_decision_tree = dt_model.transform(test_df)

predictions_decision_tree = dt_model.transform(test_df)

eval = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')    
acc = eval.evaluate(predictions_decision_tree)

sort_predictions_dt = predictions_decision_tree.select('userID', 'trackID', 'label', 'probability', 'rawPrediction', 'prediction').sort(col('userID').asc(), col('probability').desc())
sort_predictions_dt.show(6)

dt_predictions = dt_model.transform(prediction_df)    # transform prediction_df with decision tree model
dt_predictions.select('userID', 'trackID', 'probability', 'rawPrediction', 'prediction').show(12)

sort_dt_predictions = dt_predictions.select('userID', 'trackID', 'probability', 'rawPrediction', 'prediction').sort(col('userID').asc(), col('probability').desc())
sort_dt_predictions.show(6)

pd_sort_dt_predictions = sort_dt_predictions.toPandas().fillna(0.0)    # create pandas df

pd_sort_dt_predictions

columns_to_write = ['userID', 'trackID']
pd_sort_dt_predictions.to_csv('dt_predictions.csv', index=False, header=None, columns=columns_to_write)

f_dt_predictions = open('dt_predictions.csv')   
f_dt_final_predictions = open('dt_final_predictions.csv', 'w')

f_dt_final_predictions.write('TrackID,Predictor\n')

last_user_id = -1
track_id_out_vec = [0] * 6

# Go through each line of the predictions file
for line in f_dt_predictions:
    arr_out = line.strip().split(',')    # remove any spaces/new lines and create list 
    user_id_out = arr_out[0]             # set user
    track_id_out = arr_out[1]            # set track
    
    if user_id_out != last_user_id:             # if new user reached
        i = 0                                   # reset i
        
    track_id_out_vec[i] = track_id_out          # add trackID to trackID array
        
    i = i + 1                    # increment i
    last_user_id = user_id_out   # set last_user_id as current userID
    
    if i == 6:                               # if last entry for current user reached
        # Here we set the predictions 
        predictions = np.ones(shape=(6)) # initialize numpy array for predictions
        for index in range(0, 3):            
            predictions[index] = 0           # set first 3 values in array to 0 (other 3 are 1)
        
        # Here we write to the final predictions file for the 6 track predictions for the current user
        for ii in range(0, 6):         
            out_str = str(user_id_out) + '_' + str(track_id_out_vec[ii]) + ',' + str(int(predictions[ii]))
            f_dt_final_predictions.write(out_str + '\n')

f_dt_predictions.close()
f_dt_final_predictions.close()

# model 3 random forest classifier 
rf = RandomForestClassifier(featuresCol='features', labelCol='label')
rf_model = rf.fit(train_df)

predictions_rf = rf_model.transform(test_df)

sort_predictions_rf = predictions_rf.select('userID', 'trackID', 'label', 'probability', 'rawPrediction', 'prediction').sort(col('userID').asc(), col('probability').desc())
sort_predictions_rf.show(6)

rf_predictions = dt_model.transform(prediction_df)    # transform prediction_df with random forest model
rf_predictions.select('userID', 'trackID', 'probability', 'rawPrediction', 'prediction').show(12)

sort_rf_predictions = rf_predictions.select('userID', 'trackID', 'probability', 'rawPrediction', 'prediction').sort(col('userID').asc(), col('probability').desc())
sort_rf_predictions.show(6)

pd_sort_rf_predictions = sort_rf_predictions.toPandas().fillna(0.0)    # create pandas df

pd_sort_rf_predictions

columns_to_write = ['userID', 'trackID']
pd_sort_rf_predictions.to_csv('rf_predictions.csv', index=False, header=None, columns=columns_to_write)

f_rf_predictions = open('rf_predictions.csv')  
f_rf_final_predictions = open('rf_final_predictions.csv', 'w')

# Write header
f_rf_final_predictions.write('TrackID,Predictor\n')

last_user_id = -1
track_id_out_vec = [0] * 6

# Go through each line of the predictions file
for line in f_rf_predictions:
    arr_out = line.strip().split(',')    # remove any spaces/new lines and create list 
    user_id_out = arr_out[0]             # set user
    track_id_out = arr_out[1]            # set track
    
    if user_id_out != last_user_id:             # if new user reached
        i = 0                                   # reset i
        
    track_id_out_vec[i] = track_id_out          # add trackID to trackID array
        
    i = i + 1                    # increment i
    last_user_id = user_id_out   # set last_user_id as current userID
    
    if i == 6:                               # if last entry for current user reached
        # Here we set the predictions 
        predictions = np.ones(shape=(6)) # initialize numpy array for predictions
        for index in range(0, 3):            
            predictions[index] = 0           # set first 3 values in array to 0 (other 3 are 1)
        
        # Here we write to the final predictions file for the 6 track predictions for the current user
        for ii in range(0, 6):         
            out_str = str(user_id_out) + '_' + str(track_id_out_vec[ii]) + ',' + str(int(predictions[ii]))
            f_rf_final_predictions.write(out_str + '\n')

f_dt_predictions.close()
f_dt_final_predictions.close()

#model 4: Gradient Boosted Tree Classifer 
gbt = GBTClassifier(maxIter=100)
gbt_model = gbt.fit(train_df)

predictions_gbt = gbt_model.transform(test_df)

sort_predictions_gbt = predictions_gbt.select('userID', 'trackID', 'label', 'probability', 'rawPrediction', 'prediction').sort(col('userID').asc(), col('probability').desc())
sort_predictions_gbt.show(6)

gbt_predictions = gbt_model.transform(prediction_df)    # transform prediction_df with gradient-boosted tree model
gbt_predictions.select('userID', 'trackID', 'probability', 'rawPrediction', 'prediction').show(12)

sort_gbt_predictions = gbt_predictions.select('userID', 'trackID', 'probability', 'rawPrediction', 'prediction').sort(col('userID').asc(), col('probability').desc())
sort_gbt_predictions.show(6)

pd_sort_gbt_predictions = sort_gbt_predictions.toPandas().fillna(0.0)    # create pandas df

pd_sort_gbt_predictions

columns_to_write = ['userID', 'trackID']
pd_sort_gbt_predictions.to_csv('gbt_predictions.csv', index=False, header=None, columns=columns_to_write)

f_gbt_predictions = open('gbt_predictions.csv')   
f_gbt_final_predictions = open('gbt_final_predictions.csv', 'w')

f_gbt_final_predictions.write('TrackID,Predictor\n')

last_user_id = -1
track_id_out_vec = [0] * 6

# Go through each line of the predictions file
for line in f_gbt_predictions:
    arr_out = line.strip().split(',')    # remove any spaces/new lines and create list 
    user_id_out = arr_out[0]             # set user
    track_id_out = arr_out[1]            # set track
    
    if user_id_out != last_user_id:             # if new user reached
        i = 0                                   # reset i
        
    track_id_out_vec[i] = track_id_out          # add trackID to trackID array
        
    i = i + 1                    # increment i
    last_user_id = user_id_out   # set last_user_id as current userID
    
    if i == 6:                               # if last entry for current user reached
        # Here we set the predictions 
        predictions = np.ones(shape=(6)) # initialize numpy array for predictions
        for index in range(0, 3):            
            predictions[index] = 0           # set first 3 values in array to 0 (other 3 are 1)
        
        # Here we write to the final predictions file for the 6 track predictions for the current user
        for ii in range(0, 6):         
            out_str = str(user_id_out) + '_' + str(track_id_out_vec[ii]) + ',' + str(int(predictions[ii]))
            f_gbt_final_predictions.write(out_str + '\n')

f_gbt_predictions.close()
f_gbt_final_predictions.close()

